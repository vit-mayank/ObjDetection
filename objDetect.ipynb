{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 1: Setup Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import torch\n",
    "from torchreid.utils import FeatureExtractor\n",
    "import time\n",
    "import sqlite3\n",
    "import logging\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "logging.basicConfig(filename='debug.log', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 2: Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLOv8 model\n",
    "model = YOLO(\"yolov9s.pt\")\n",
    "\n",
    "# Initialize DeepSORT tracker\n",
    "tracker = DeepSort(max_age=30, nn_budget=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 3: Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Amrit Baskota\\anaconda3\\envs\\objdetect\\Lib\\site-packages\\torchreid\\utils\\tools.py:43: UserWarning: No file found at \"osnet_x1_0.pth\"\n",
      "  warnings.warn('No file found at \"{}\"'.format(fpath))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded imagenet pretrained weights from \"C:\\Users\\Amrit Baskota/.cache\\torch\\checkpoints\\osnet_x1_0_imagenet.pth\"\n",
      "** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']\n",
      "Model: osnet_x1_0\n",
      "- params: 2,193,616\n",
      "- flops: 978,878,352\n"
     ]
    }
   ],
   "source": [
    "# Load ReID model (OSNet)\n",
    "extractor = FeatureExtractor(\n",
    "    model_name='osnet_x1_0',\n",
    "    model_path='osnet_x1_0.pth',  # Ensure you have the model weights\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 3: Define Video Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define video sources\n",
    "video_sources = [\"video1.mp4\", \"video2.mp4\"]  # Replace with actual camera feeds\n",
    "caps = [cv2.VideoCapture(src) for src in video_sources]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 4: Global Tracking Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global tracking dictionary {global_id: {\"features\": (color_hist, body_size), \"last_seen\": timestamp, \"camera\": cam_id}}\n",
    "global_tracks = {}\n",
    "\n",
    "# Store rectangles for click detection\n",
    "rectangles = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 5: Mouse Click Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mouse callback function to handle rectangle click\n",
    "def click_callback(event, x, y, flags, param):\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        for global_id, rect in rectangles.items():\n",
    "            x1, y1, x2, y2, color = rect\n",
    "            if x1 <= x <= x2 and y1 <= y <= y2:\n",
    "                new_color = (0, 255, 0) if color == (0, 0, 255) else (0, 0, 255)\n",
    "                rectangles[global_id] = (x1, y1, x2, y2, new_color)\n",
    "                logging.debug(f\"Clicked ID {global_id}, Color changed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 6: Database Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to SQLite database\n",
    "def connect_db():\n",
    "    return sqlite3.connect('person_tracking.db')\n",
    "\n",
    "# Store person information in the database\n",
    "def store_person_info(global_id, features, body_size, last_seen, location):\n",
    "    conn = sqlite3.connect('person_data.db')\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute('''CREATE TABLE IF NOT EXISTS persons (\n",
    "                        global_id INTEGER PRIMARY KEY, \n",
    "                        color_hist BLOB, \n",
    "                        body_size REAL, \n",
    "                        last_seen REAL, \n",
    "                        x1 REAL, y1 REAL, x2 REAL, y2 REAL)''')\n",
    "\n",
    "    cursor.execute('''INSERT OR REPLACE INTO persons \n",
    "                      (global_id, color_hist, body_size, last_seen, x1, y1, x2, y2) \n",
    "                      VALUES (?, ?, ?, ?, ?, ?, ?, ?)''', \n",
    "                   (global_id, features, body_size, last_seen, *location))\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 7: Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract deep features for ReID\n",
    "def extract_deep_features(image, bbox):\n",
    "    x1, y1, x2, y2 = map(int, bbox)\n",
    "    person_crop = image[y1:y2, x1:x2]\n",
    "    person_crop = cv2.resize(person_crop, (128, 256))\n",
    "    person_crop = np.transpose(person_crop, (2, 0, 1)) / 255.0\n",
    "    person_crop = torch.tensor(person_crop, dtype=torch.float32).unsqueeze(0)\n",
    "    features = extractor(person_crop)\n",
    "    return features[0].cpu().detach().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 8: Person Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matching_person(new_feature, cam_id, threshold=0.4):\n",
    "    best_match = None\n",
    "    best_score = float(\"inf\")\n",
    "\n",
    "    for track_id, data in global_tracks.items():\n",
    "        if data[\"camera\"] == cam_id:\n",
    "            continue  # Skip if the person is already in this camera\n",
    "\n",
    "        old_feature = data[\"features\"]\n",
    "        score = cosine(new_feature, old_feature)\n",
    "\n",
    "        if score < threshold and score < best_score:\n",
    "            best_score = score\n",
    "            best_match = track_id\n",
    "\n",
    "    return best_match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 9: Video Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each video stream\n",
    "def process_video(src, cam_id):\n",
    "    cap = cv2.VideoCapture(video_sources[cam_id])\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Couldn't open video {video_sources[cam_id]}\")\n",
    "        return\n",
    "\n",
    "    # Create a VideoWriter object to save the processed video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # You can use other codecs like 'MP4V' or 'MJPG'\n",
    "    out = cv2.VideoWriter(f\"output_{cam_id}.mp4\", fourcc, 20.0, (640, 480))  # Change resolution if needed\n",
    "\n",
    "    window_name = f\"Camera {cam_id}\"\n",
    "    cv2.namedWindow(window_name)\n",
    "    cv2.setMouseCallback(window_name, click_callback)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  \n",
    "\n",
    "        results = model(frame)\n",
    "        detections = []\n",
    "        track_map = {}\n",
    "\n",
    "        for r in results:\n",
    "            for box in r.boxes:\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                conf = float(box.conf[0])\n",
    "                cls = int(box.cls[0])\n",
    "\n",
    "                if cls == 0 and conf > 0.6:\n",
    "                    bbox = [x1, y1, x2-x1, y2-y1]\n",
    "                    deep_features = extract_deep_features(frame, (x1, y1, x2, y2))\n",
    "                    \n",
    "                    matched_id = find_matching_person(deep_features, cam_id)\n",
    "\n",
    "                    if matched_id is None:\n",
    "                        global_id = len(global_tracks) + 1\n",
    "                        global_tracks[global_id] = {\"features\": deep_features, \"last_seen\": time.time(), \"camera\": cam_id}\n",
    "                    else:\n",
    "                        global_id = matched_id\n",
    "                        global_tracks[global_id][\"last_seen\"] = time.time()\n",
    "                        global_tracks[global_id][\"camera\"] = cam_id\n",
    "\n",
    "                    track_map[len(detections)] = global_id\n",
    "                    detections.append((bbox, conf, \"person\"))\n",
    "\n",
    "        tracks = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "        for i, track in enumerate(tracks):\n",
    "            if track.is_confirmed():\n",
    "                bbox = track.to_tlbr()\n",
    "                local_id = track.track_id\n",
    "                global_id = track_map.get(local_id, local_id)\n",
    "\n",
    "                color = rectangles.get(global_id, (0, 0, 0, 0, (0, 255, 0)))[4]\n",
    "                cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), color, 2)\n",
    "                cv2.putText(frame, f\"ID {global_id} (Cam {cam_id})\", (int(bbox[0]), int(bbox[1]) - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "                rectangles[global_id] = (int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3]), color)\n",
    "\n",
    "        # Write the processed frame to the video file\n",
    "        out.write(frame)\n",
    "\n",
    "        # Display the frame in a window\n",
    "        cv2.imshow(window_name, frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()  # Release the VideoWriter\n",
    "    cv2.destroyWindow(window_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 10: Run Video Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the video processing for each camera sequentially\n",
    "for i, cap in enumerate(caps):\n",
    "    process_video(cap, i)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "objdetect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
